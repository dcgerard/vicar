% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mouthwash.R
\name{mouthwash}
\alias{mouthwash}
\title{MOUTHWASH: Maximize Over Unobservables To Help With Adaptive SHrinkage.}
\usage{
mouthwash(Y, X, k = NULL, cov_of_interest = ncol(X),
  include_intercept = TRUE, limmashrink = TRUE, fa_func = pca_naive,
  fa_args = list(), likelihood = c("normal", "t"),
  mixing_dist = c("normal", "uniform", "+uniform", "sym_uniform", "normal"),
  lambda_type = c("zero_conc", "uniform"), pi_init_type = c("zero_conc",
  "uniform", "random"), degrees_freedom = NULL, pi_init = NULL,
  grid_seq = NULL, lambda_seq = NULL, lambda0 = 10, scale_var = TRUE,
  plot_update = FALSE)
}
\arguments{
\item{Y}{A matrix of numerics. These are the response variables
where each column has its own variance. In a gene expression
study, the rows are the individuals and the columns are the
genes.}

\item{X}{A matrix of numerics. The covariates of interest.}

\item{k}{A non-negative integer.The number of unobserved
confounders. If not specified and the R package sva is
installed, then this function will estimate the number of
hidden confounders using the methods of Buja and Eyuboglu
(1992).}

\item{cov_of_interest}{A vector of positive integers. The column
numbers of the covariates in X whose coefficients you are
interested in. The rest are considered nuisance parameters and
are regressed out by OLS.}

\item{include_intercept}{A logical. If \code{TRUE}, then it will
check \code{X} to see if it has an intercept term. If not, then
it will add an intercept term. If \code{FALSE}, then \code{X}
will be unchanged.}

\item{limmashrink}{A logical. Should we apply hierarchical
shrinkage to the variances (\code{TRUE}) or not (\code{FALSE})?
If \code{degrees_freedom = NULL} and \code{limmashrink = TRUE}
and \code{likelihood = "t"}, then we'll also use the limma
returned degrees of freedom.}

\item{fa_func}{A factor analysis function. The function must have
as inputs a numeric matrix \code{Y} and a rank (numeric scalar)
\code{r}. It must output numeric matrices \code{alpha} and
\code{Z} and a numeric vector \code{sig_diag}. \code{alpha} is
the estimate of the coefficients of the unobserved confounders,
so it must be an \code{r} by \code{ncol(Y)} matrix. \code{Z}
must be an \code{r} by \code{nrow(Y)} matrix. \code{sig_diag}
is the estimate of the column-wise variances so it must be of
length \code{ncol(Y)}. The default is the function
\code{pca_naive} that just uses the first \code{r} singular
vectors as the estimate of \code{alpha}. The estimated
variances are just the column-wise mean square.}

\item{fa_args}{A list. Additional arguments you want to pass to
fa_func.}

\item{likelihood}{Either \code{"normal"} or \code{"t"}. If
\code{likelihood = "t"}, then the user may provide the degrees
of freedom via \code{degrees_freedom}.}

\item{mixing_dist}{A character. Should we use a mixture of uniforms
(\code{"uniform"}), a mixture of uniforms with minimum at 0
(\code{"+uniform"}), a mixture of uniforms symmetric at 0
(\code{"sym_uniform"}), or a mixture of normals
(\code{"normal"})?}

\item{lambda_type}{A character. Should we apply a penalty on zero
(\code{"zero_conc"}) or no penalty (\code{"uniform"}). Not used
if \code{lambda_seq} is not \code{NULL}.}

\item{pi_init_type}{How should we initialize the mixing
proportions? By concentrating on zero (\code{"zero_conc"}), by
equal weights on all mixing distributions (\code{"uniform"}),
or by sampling uniformly on the simplex (\code{"random"})?}

\item{degrees_freedom}{if \code{likelihood = "t"}, then this is the
user-defined degrees of freedom for that distribution. If
\code{degrees_freedom} is \code{NULL} then the degrees of
freedom will be the sample size minus the number of covariates
minus \code{k}.}

\item{pi_init}{A numeric vector. These are the initial values of
the mixing proportions.}

\item{grid_seq}{The grid for the mixing distribution. If
\code{mixing_dist = "uniform"} or \code{"+uniform"}, then these
should be the non-zero limits of the uniform distributions. If
\code{mixing_dist = "sym_uniform"}, then these should be the
right limits of the uniform distributions. If \code{mixing_dist
= "normal"}, then these should be the variances of the mixing
normal distributions.}

\item{lambda_seq}{A numeric vector with elements all greater than
or equal to 1. These are the tuning parameters for the mixing
proportions. This can only be specified if \code{grid_seq} is
also specified.}

\item{lambda0}{A numeric greater than or equal to 1. The penalty on
zero if \code{lambda_type = "zero_conc"}.}

\item{scale_var}{A logical. Should we estimate a variance inflation
parameter (\code{TRUE}) or not (\code{FALSE})?}

\item{plot_update}{A logical. Should I plot the the path of the
log-likelihood (\code{TRUE}) or not (\code{FALSE})? Only
applicable when \code{mixing_dist} is not \code{"normal"}.}
}
\value{
A list with some or all of the following elements.

    \code{fitted_g}: The estimated unimodal prior. It is of class
    \code{\link[ashr]{unimix}} if \code{mixing_dist} is one of
    \code{"uniform"}, \code{"+uniform"}, or
    \code{"sym_uniform"}. It is of class
    \code{\link[ashr]{normalmix}} if \code{mixing_dist} is
    \code{"normal"}.

    \code{loglik} The final log-likelihood.

    \code{logLR} The likelihood ratio compared to the all-null setting (point-mass on zero).

    \code{data} Post-confounder adjusted ashr data.

    \code{pi0} The estimate of the proportion of null genes.

    \code{z2} The estimated confounders (after rotation)
    corresponding the covariates of interest. Mostly output for
    debugging reasons.

    \code{xi} The estimated variance inflation parameter.

    \code{Zhat} The estimate of the confounders.

    \code{alphahat} The estimate of the coefficients of the confounders.

    \code{sig_diag} The estimate of the column-specific variances.

    \code{result} A data frame with the results from MOUTHWASH. The columns of which are
    \itemize{
      \item{NegativeProb}{The probability that the effect is negative.}
      \item{PositiveProb}{The probability that the effect is positive.}
      \item{lfsr}{The local false sign rates of each effect.}
      \item{svalue}{The s-values, a measure of significance.}
      \item{lfdr}{The local false discovery rates.}
      \item{qvalue}{Teh q-values, a measure of significance.}
      \item{PosteriorMean}{The posterior means of the effects.}
      \item{PosteriorSD}{The posterior standard deviations of the effects.}
    }
}
\description{
This function implements the full MOUTHWASH method. First, it
rotates the response and explanatory variables into a part that we
use to estimate the confounding variables and the variances, and a
part that we use to estimate the coefficients of the observed
covariates. This function will implement a factor analysis for the
first part then run \code{\link{mouthwash_second_step}} for the
second part.
}
\details{
The assumed mode is \deqn{Y = X\beta + Z\alpha + E.} \eqn{Y} is a
\eqn{n} by \code{p} matrix of response varaibles. For example, each
row might be an array of log-transformed gene-expression data.
\eqn{X} is a \eqn{n} by \eqn{q} matrix of observed covariates. It
is assumed that all but one column of which contains nuisance
parameters. For example, the first column might be a vector of ones
to include an intercept. \eqn{\beta} is a \eqn{q} by \eqn{p} matrix
of corresponding coefficients.  \eqn{Z} is a \eqn{n} by \eqn{k}
matrix of confounder variables. \eqn{\alpha} is the corresponding
\eqn{k} by \eqn{p} matrix of coefficients for the unobserved
confounders. \eqn{E} is a \eqn{n} by \eqn{p} matrix of error
terms. \eqn{E} is assumed to be matrix normal with identity row
covariance and diagonal column covariance \eqn{\Sigma}. That is,
the columns are heteroscedastic while the rows are homoscedastic
independent.

This function will first rotate \eqn{Y} and \eqn{X} using the QR
decomposition. This separates the model into three parts. The first
part only contains nuisance parameters, the second part contains
the coefficients of interest, and the third part contains the
confounders. \code{mouthwash} applies a user-provided factor
analysis to the third part to estimate the confounding factors,
then runs an EM (or coordinate-ascent) algorithm on the second part
to estimate the coefficients of interest.

Many forms of factor analyses are avaiable in this package. The
default is PCA with the column-wise residual mean-squares as the
estimates of the column-wise variances.
}
\author{
David Gerard
}

